{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import os\n",
    "from PIL import Image\n",
    "from torchvision.models.detection import (\n",
    "    retinanet_resnet50_fpn,\n",
    "    RetinaNet_ResNet50_FPN_Weights,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torchvision.ops import box_iou\n",
    "from torch.optim.lr_scheduler import OneCycleLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_LIMIT = 4000\n",
    "VAL_LIMIT = 500\n",
    "TEST_LIMIT = 500\n",
    "LR = 0.001\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 0.0001\n",
    "BATCH_SIZE = 2\n",
    "NUM_EPOCHS = 10\n",
    "PATIENCE = 5\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DocBankDataset Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DocBankDataset(torch.utils.data.Dataset):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         idx_file_path,\n",
    "#         limit,\n",
    "#         images_dir=\"/content/drive/MyDrive/dataset/images_yolo\",\n",
    "#         labels_dir=\"/content/drive/MyDrive/dataset/labels_retina\",\n",
    "#         transforms=None,\n",
    "#     ):\n",
    "#         self.images = []\n",
    "#         self.annotations = []\n",
    "#         self.transforms = transforms\n",
    "#         self.images_dir = images_dir\n",
    "#         self.labels_dir = labels_dir\n",
    "#         self.idx_file_path = idx_file_path\n",
    "#         self.limit = limit\n",
    "#         self.load_images()\n",
    "#         # print(\"Images loaded\")\n",
    "#         self.load_ann()\n",
    "#         # print(\"Annotations loaded\")\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         img = self.images[idx]\n",
    "#         target = self.annotations[idx]\n",
    "#         return img, target\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.images)\n",
    "\n",
    "#     def load_images(self):\n",
    "#         with open(self.idx_file_path, \"r\") as f:\n",
    "#             lines = f.readlines()\n",
    "#             for line in lines:\n",
    "#                 if len(self.images) == self.limit:\n",
    "#                     return\n",
    "#                 img_path = os.path.join(self.images_dir, line[:-5] + \"_ori.jpg\")\n",
    "#                 # print(\"read path {}\".format(img_path))\n",
    "#                 img = Image.open(img_path).convert(\"RGB\")\n",
    "#                 if self.transforms is not None:\n",
    "#                     img = self.transforms(img)\n",
    "#                 self.images.append(img)\n",
    "\n",
    "#     def load_ann(self):\n",
    "#         with open(self.idx_file_path, \"r\") as f:\n",
    "#             files = f.readlines()\n",
    "#             for file in files:\n",
    "#                 if len(self.annotations) == self.limit:\n",
    "#                     return\n",
    "#                 ann_path = os.path.join(self.labels_dir, file[:-5] + \".txt\")\n",
    "#                 self.annotations.append(self.process_ann_path(ann_path))\n",
    "\n",
    "#     def process_ann_path(self, ann_path):\n",
    "#         # return dictionary with 2 key: boxes (FloatTensor[N, 4]) and labels (Int64Tensor[N])\n",
    "#         target = {}\n",
    "#         boxes = []\n",
    "#         labels = []\n",
    "#         with open(ann_path, \"r\") as f:\n",
    "#             lines = f.readlines()\n",
    "#             for line in lines:\n",
    "#                 content = line.strip().split(\"\\t\")\n",
    "#                 token, x0, y0, x1, y1, R, G, B, font, label = content\n",
    "\n",
    "#                 # MAY NEED BOX DIMENSION VALIDATION CHECK HERE\n",
    "#                 if (\n",
    "#                     (int(x0) < 0)\n",
    "#                     or (int(y0) < 0)\n",
    "#                     or (int(x1) < 0)\n",
    "#                     or (int(y1) < 0)\n",
    "#                     or (x1 <= x0 or y1 <= y0)\n",
    "#                 ):\n",
    "#                     continue\n",
    "\n",
    "#                 boxes.append([float(x0), float(y0), float(x1), float(y1)])\n",
    "#                 if label == \"figure\":\n",
    "#                     labels.append(torch.tensor(1))\n",
    "#                 else:\n",
    "#                     labels.append(torch.tensor(0))\n",
    "\n",
    "#         target[\"boxes\"] = torch.FloatTensor(boxes)\n",
    "#         target[\"labels\"] = torch.tensor(labels)\n",
    "#         # print(target)\n",
    "#         return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocBankDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        idx_file_path,\n",
    "        limit,\n",
    "        images_dir,\n",
    "        labels_dir,\n",
    "        transforms=None,\n",
    "    ):\n",
    "        self.transforms = transforms\n",
    "        self.images_dir = images_dir\n",
    "        self.labels_dir = labels_dir\n",
    "        self.limit = limit\n",
    "\n",
    "        with open(idx_file_path, \"r\") as f:\n",
    "            self.file_list = [line.strip() for line in f.readlines()[:limit]]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.file_list[idx]\n",
    "        img_path = os.path.join(self.images_dir, file_name[:-4] + \"_ori.jpg\")\n",
    "        ann_path = os.path.join(self.labels_dir, file_name[:-4] + \".txt\")\n",
    "\n",
    "        # Load image\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        # Load annotation\n",
    "        target = self.process_ann_path(ann_path)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def process_ann_path(self, ann_path):\n",
    "        target = {}\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        with open(ann_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                content = line.strip().split(\"\\t\")\n",
    "                token, x0, y0, x1, y1, R, G, B, font, label = content\n",
    "\n",
    "                if (\n",
    "                    (int(x0) < 0)\n",
    "                    or (int(y0) < 0)\n",
    "                    or (int(x1) < 0)\n",
    "                    or (int(y1) < 0)\n",
    "                    or (x1 <= x0 or y1 <= y0)\n",
    "                ):\n",
    "                    continue\n",
    "\n",
    "                boxes.append([float(x0), float(y0), float(x1), float(y1)])\n",
    "                labels.append(1 if label == \"figure\" else 0)\n",
    "\n",
    "        target[\"boxes\"] = torch.FloatTensor(boxes)\n",
    "        target[\"labels\"] = torch.tensor(labels)\n",
    "        return target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directory Definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dir = \"D:/Docbank/images\"\n",
    "labels_dir = \"D:/Docbank/annotations\"\n",
    "train_idx_file_path = \"D:/Docbank/indexed/500K_train.txt\"\n",
    "val_idx_file_path = \"D:/Docbank/indexed/500K_dev.txt\"\n",
    "test_idx_file_path = \"D:/Docbank/indexed/500K_test.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Val, Test Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = transforms.Compose(\n",
    "    [transforms.Resize((1000, 1000)), transforms.ToTensor()]  # Resize to a square\n",
    ")\n",
    "\n",
    "train_dataset = DocBankDataset(\n",
    "    train_idx_file_path, TRAIN_LIMIT, images_dir, labels_dir, transforms\n",
    ")\n",
    "val_dataset = DocBankDataset(\n",
    "    val_idx_file_path, VAL_LIMIT, images_dir, labels_dir, transforms\n",
    ")\n",
    "test_dataset = DocBankDataset(\n",
    "    test_idx_file_path, TEST_LIMIT, images_dir, labels_dir, transforms\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Val, Test Dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images, targets = zip(*batch)\n",
    "    return images, targets\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True,\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = retinanet_resnet50_fpn(num_classes=2)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer and LR Scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "# lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "#     optimizer, milestones=[4, 8], gamma=0.1\n",
    "# )\n",
    "\n",
    "lr_scheduler = OneCycleLR(\n",
    "    optimizer, max_lr=0.01, epochs=NUM_EPOCHS, steps_per_epoch=len(train_loader)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = GradScaler()\n",
    "\n",
    "\n",
    "def train(model, optimizer, lr_scheduler, train_loader, val_loader, num_epochs, device):\n",
    "    best_f1_score = -float(\"inf\")\n",
    "    patience_counter = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        print(f\"Current GPU memory usage: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "        for images, targets in tqdm(train_loader, desc=\"Training\", leave=False):\n",
    "            images = list(image.to(device) for image in images)\n",
    "            for t in targets:\n",
    "                boxes = t[\"boxes\"]\n",
    "\n",
    "                # Check if boxes are empty\n",
    "                if boxes.shape[0] == 0:\n",
    "                    print(\"Skipping image with no ground truth boxes.\")\n",
    "                    continue  # Skip to the next iteration\n",
    "\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # # Forward pass\n",
    "            # loss_dict = model(images, targets)\n",
    "            # losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "            # # Backward pass and optimization\n",
    "            # losses.backward()\n",
    "            # optimizer.step()\n",
    "            # train_loss += losses.item()\n",
    "\n",
    "            with autocast(device_type=\"cuda\"):\n",
    "                loss_dict = model(images, targets)\n",
    "                losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "            scaler.scale(losses).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "        # Step the learning rate scheduler\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        f1_score = 0\n",
    "        with torch.no_grad():\n",
    "            for images, targets in tqdm(val_loader, desc=\"Validating\", leave=False):\n",
    "                images = list(image.to(device) for image in images)\n",
    "\n",
    "                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "                # Forward pass\n",
    "                predictions = model(images)\n",
    "                TP, FP, FN = 0, 0, 0\n",
    "                iou_threshold = 0.5\n",
    "\n",
    "                for target, prediction in zip(targets, predictions):\n",
    "                    target_boxes = target[\"boxes\"].cpu()\n",
    "                    predicted_boxes = prediction[\"boxes\"].cpu()\n",
    "\n",
    "                    # Skip this iteration if there are no target boxes\n",
    "                    if target_boxes.shape[0] == 0:\n",
    "                        print(\"Skipping image with no ground truth boxes.\")\n",
    "                        continue  # Skip to the next iteration\n",
    "                    # Ensure target_boxes is 2D\n",
    "                    if target_boxes.ndim == 1:  # If only one target box\n",
    "                        target_boxes = target_boxes.unsqueeze(0)  # Make it [1, 4]\n",
    "                    # Ensure predicted_boxes is 2D\n",
    "                    if predicted_boxes.ndim == 1:  # If only one predicted box\n",
    "                        predicted_boxes = predicted_boxes.unsqueeze(0)  # Make it [1, 4]\n",
    "\n",
    "                    # print(target_boxes.shape, predicted_boxes.shape)\n",
    "                    iou = box_iou(predicted_boxes, target_boxes)\n",
    "                    for i in range(predicted_boxes.shape[0]):\n",
    "                        if (iou[i] > iou_threshold).any():\n",
    "                            TP += 1  # True Positive\n",
    "                        else:\n",
    "                            FP += 1  # False Positive\n",
    "                    # Count False Negatives\n",
    "                    FN += len(target_boxes) - TP\n",
    "\n",
    "        # DocBank Metrics\n",
    "        precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "        recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "        f1_score = (\n",
    "            2 * (precision * recall) / (precision + recall)\n",
    "            if (precision + recall) > 0\n",
    "            else 0\n",
    "        )\n",
    "\n",
    "        # Calculate average losses\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # Print epoch results\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"F1 Score: {f1_score:.4f}\")\n",
    "        print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "        # Save the best model\n",
    "        if f1_score > best_f1_score:\n",
    "            best_f1_score = f1_score\n",
    "            torch.save(model.state_dict(), \"best_retina.pth\")\n",
    "            print(\"Saved new best model\")\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "        # if f1_score > best_f1_score:\n",
    "        #     best_f1_score = f1_score\n",
    "        #     torch.save(model.state_dict(), \"best_retina.pth\")\n",
    "        #     print(\"Saved new best model\")\n",
    "\n",
    "    print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, optimizer, lr_scheduler, train_loader, val_loader, NUM_EPOCHS, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
