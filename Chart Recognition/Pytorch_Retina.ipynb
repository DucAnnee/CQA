{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import os\n",
    "from PIL import Image\n",
    "from torchvision.models.detection import (\n",
    "    retinanet_resnet50_fpn,\n",
    "    RetinaNet_ResNet50_FPN_Weights,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torchvision.ops import box_iou\n",
    "from torch.optim.lr_scheduler import OneCycleLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "TRAIN_LIMIT = 40000\n",
    "VAL_LIMIT = 5000\n",
    "TEST_LIMIT = 5000\n",
    "LR = 0.001\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 0.0001\n",
    "BATCH_SIZE = 2\n",
    "NUM_EPOCHS = 10\n",
    "PATIENCE = 5\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DocBankDataset Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DocBankDataset(torch.utils.data.Dataset):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         idx_file_path,\n",
    "#         limit,\n",
    "#         images_dir=\"/content/drive/MyDrive/dataset/images_yolo\",\n",
    "#         labels_dir=\"/content/drive/MyDrive/dataset/labels_retina\",\n",
    "#         transforms=None,\n",
    "#     ):\n",
    "#         self.images = []\n",
    "#         self.annotations = []\n",
    "#         self.transforms = transforms\n",
    "#         self.images_dir = images_dir\n",
    "#         self.labels_dir = labels_dir\n",
    "#         self.idx_file_path = idx_file_path\n",
    "#         self.limit = limit\n",
    "#         self.load_images()\n",
    "#         # print(\"Images loaded\")\n",
    "#         self.load_ann()\n",
    "#         # print(\"Annotations loaded\")\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         img = self.images[idx]\n",
    "#         target = self.annotations[idx]\n",
    "#         return img, target\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.images)\n",
    "\n",
    "#     def load_images(self):\n",
    "#         with open(self.idx_file_path, \"r\") as f:\n",
    "#             lines = f.readlines()\n",
    "#             for line in lines:\n",
    "#                 if len(self.images) == self.limit:\n",
    "#                     return\n",
    "#                 img_path = os.path.join(self.images_dir, line[:-5] + \"_ori.jpg\")\n",
    "#                 # print(\"read path {}\".format(img_path))\n",
    "#                 img = Image.open(img_path).convert(\"RGB\")\n",
    "#                 if self.transforms is not None:\n",
    "#                     img = self.transforms(img)\n",
    "#                 self.images.append(img)\n",
    "\n",
    "#     def load_ann(self):\n",
    "#         with open(self.idx_file_path, \"r\") as f:\n",
    "#             files = f.readlines()\n",
    "#             for file in files:\n",
    "#                 if len(self.annotations) == self.limit:\n",
    "#                     return\n",
    "#                 ann_path = os.path.join(self.labels_dir, file[:-5] + \".txt\")\n",
    "#                 self.annotations.append(self.process_ann_path(ann_path))\n",
    "\n",
    "#     def process_ann_path(self, ann_path):\n",
    "#         # return dictionary with 2 key: boxes (FloatTensor[N, 4]) and labels (Int64Tensor[N])\n",
    "#         target = {}\n",
    "#         boxes = []\n",
    "#         labels = []\n",
    "#         with open(ann_path, \"r\") as f:\n",
    "#             lines = f.readlines()\n",
    "#             for line in lines:\n",
    "#                 content = line.strip().split(\"\\t\")\n",
    "#                 token, x0, y0, x1, y1, R, G, B, font, label = content\n",
    "\n",
    "#                 # MAY NEED BOX DIMENSION VALIDATION CHECK HERE\n",
    "#                 if (\n",
    "#                     (int(x0) < 0)\n",
    "#                     or (int(y0) < 0)\n",
    "#                     or (int(x1) < 0)\n",
    "#                     or (int(y1) < 0)\n",
    "#                     or (x1 <= x0 or y1 <= y0)\n",
    "#                 ):\n",
    "#                     continue\n",
    "\n",
    "#                 boxes.append([float(x0), float(y0), float(x1), float(y1)])\n",
    "#                 if label == \"figure\":\n",
    "#                     labels.append(torch.tensor(1))\n",
    "#                 else:\n",
    "#                     labels.append(torch.tensor(0))\n",
    "\n",
    "#         target[\"boxes\"] = torch.FloatTensor(boxes)\n",
    "#         target[\"labels\"] = torch.tensor(labels)\n",
    "#         # print(target)\n",
    "#         return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocBankDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        idx_file_path,\n",
    "        limit,\n",
    "        images_dir,\n",
    "        labels_dir,\n",
    "        transforms=None,\n",
    "    ):\n",
    "        self.transforms = transforms\n",
    "        self.images_dir = images_dir\n",
    "        self.labels_dir = labels_dir\n",
    "        self.limit = limit\n",
    "\n",
    "        with open(idx_file_path, \"r\") as f:\n",
    "            self.file_list = [line.strip() for line in f.readlines()[:limit]]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.file_list[idx]\n",
    "        img_path = os.path.join(self.images_dir, file_name[:-4] + \"_ori.jpg\")\n",
    "        ann_path = os.path.join(self.labels_dir, file_name[:-4] + \".txt\")\n",
    "\n",
    "        # Load image\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        # Load annotation\n",
    "        target = self.process_ann_path(ann_path)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def process_ann_path(self, ann_path):\n",
    "        target = {}\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        with open(ann_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                content = line.strip().split(\"\\t\")\n",
    "                token, x0, y0, x1, y1, R, G, B, font, label = content\n",
    "\n",
    "                if (\n",
    "                    (int(x0) < 0)\n",
    "                    or (int(y0) < 0)\n",
    "                    or (int(x1) < 0)\n",
    "                    or (int(y1) < 0)\n",
    "                    or (x1 <= x0 or y1 <= y0)\n",
    "                ):\n",
    "                    continue\n",
    "\n",
    "                boxes.append([float(x0), float(y0), float(x1), float(y1)])\n",
    "                labels.append(1 if label == \"figure\" else 0)\n",
    "\n",
    "        target[\"boxes\"] = torch.FloatTensor(boxes)\n",
    "        target[\"labels\"] = torch.tensor(labels)\n",
    "        return target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directory Definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dir = \"D:/Docbank/images\"\n",
    "labels_dir = \"D:/Docbank/annotations\"\n",
    "train_idx_file_path = \"D:/Docbank/indexed/500K_train.txt\"\n",
    "val_idx_file_path = \"D:/Docbank/indexed/500K_dev.txt\"\n",
    "test_idx_file_path = \"D:/Docbank/indexed/500K_test.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Val, Test Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = transforms.Compose(\n",
    "    [transforms.Resize((1000, 1000)), transforms.ToTensor()]  # Resize to a square\n",
    ")\n",
    "\n",
    "train_dataset = DocBankDataset(\n",
    "    train_idx_file_path, TRAIN_LIMIT, images_dir, labels_dir, transforms\n",
    ")\n",
    "val_dataset = DocBankDataset(\n",
    "    val_idx_file_path, VAL_LIMIT, images_dir, labels_dir, transforms\n",
    ")\n",
    "test_dataset = DocBankDataset(\n",
    "    test_idx_file_path, TEST_LIMIT, images_dir, labels_dir, transforms\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Val, Test Dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images, targets = zip(*batch)\n",
    "    return images, targets\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True,\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RetinaNet(\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0-2): 3 x Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelP6P7(\n",
       "        (p6): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (p7): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (anchor_generator): AnchorGenerator()\n",
       "  (head): RetinaNetHead(\n",
       "    (classification_head): RetinaNetClassificationHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (regression_head): RetinaNetRegressionHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (bbox_reg): Conv2d(256, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = retinanet_resnet50_fpn(num_classes=2)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer and LR Scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "# lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "#     optimizer, milestones=[4, 8], gamma=0.1\n",
    "# )\n",
    "\n",
    "lr_scheduler = OneCycleLR(\n",
    "    optimizer, max_lr=0.01, epochs=NUM_EPOCHS, steps_per_epoch=len(train_loader)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = GradScaler()\n",
    "\n",
    "\n",
    "def train(model, optimizer, lr_scheduler, train_loader, val_loader, num_epochs, device):\n",
    "    best_f1_score = -float(\"inf\")\n",
    "    patience_counter = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        print(f\"Current GPU memory usage: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "        for images, targets in tqdm(train_loader, desc=\"Training\", leave=False):\n",
    "            images = list(image.to(device) for image in images)\n",
    "            for t in targets:\n",
    "                boxes = t[\"boxes\"]\n",
    "                if boxes.ndim == 1:  # Ensure single box case is handled\n",
    "                    t[\"boxes\"] = boxes.unsqueeze(0)\n",
    "\n",
    "                assert (\n",
    "                    t[\"boxes\"].shape[1] == 4\n",
    "                ), \"Bounding boxes should be of shape [N, 4]\"\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # # Forward pass\n",
    "            # loss_dict = model(images, targets)\n",
    "            # losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "            # # Backward pass and optimization\n",
    "            # losses.backward()\n",
    "            # optimizer.step()\n",
    "            # train_loss += losses.item()\n",
    "\n",
    "            with autocast(device_type=\"cuda\"):\n",
    "                loss_dict = model(images, targets)\n",
    "                losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "            scaler.scale(losses).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "        # Step the learning rate scheduler\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        f1_score = 0\n",
    "        with torch.no_grad():\n",
    "            for images, targets in tqdm(val_loader, desc=\"Validating\", leave=False):\n",
    "                images = list(image.to(device) for image in images)\n",
    "\n",
    "                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "                # Forward pass\n",
    "                predictions = model(images)\n",
    "                TP, FP, FN = 0, 0, 0\n",
    "                iou_threshold = 0.5\n",
    "\n",
    "                for target, prediction in zip(targets, predictions):\n",
    "                    target_boxes = target[\"boxes\"].cpu()\n",
    "                    predicted_boxes = prediction[\"boxes\"].cpu()\n",
    "\n",
    "                    # Skip this iteration if there are no target boxes\n",
    "                    if target_boxes.shape[0] == 0:\n",
    "                        print(\"Skipping image with no ground truth boxes.\")\n",
    "                        continue  # Skip to the next iteration\n",
    "                    # Ensure target_boxes is 2D\n",
    "                    if target_boxes.ndim == 1:  # If only one target box\n",
    "                        target_boxes = target_boxes.unsqueeze(0)  # Make it [1, 4]\n",
    "                    # Ensure predicted_boxes is 2D\n",
    "                    if predicted_boxes.ndim == 1:  # If only one predicted box\n",
    "                        predicted_boxes = predicted_boxes.unsqueeze(0)  # Make it [1, 4]\n",
    "\n",
    "                    # print(target_boxes.shape, predicted_boxes.shape)\n",
    "                    iou = box_iou(predicted_boxes, target_boxes)\n",
    "                    for i in range(predicted_boxes.shape[0]):\n",
    "                        if (iou[i] > iou_threshold).any():\n",
    "                            TP += 1  # True Positive\n",
    "                        else:\n",
    "                            FP += 1  # False Positive\n",
    "                    # Count False Negatives\n",
    "                    FN += len(target_boxes) - TP\n",
    "\n",
    "        # DocBank Metrics\n",
    "        precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "        recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "        f1_score = (\n",
    "            2 * (precision * recall) / (precision + recall)\n",
    "            if (precision + recall) > 0\n",
    "            else 0\n",
    "        )\n",
    "\n",
    "        # Calculate average losses\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # Print epoch results\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"F1 Score: {f1_score:.4f}\")\n",
    "        print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "        # Save the best model\n",
    "        if f1_score > best_f1_score:\n",
    "            best_f1_score = f1_score\n",
    "            torch.save(model.state_dict(), \"best_retina.pth\")\n",
    "            print(\"Saved new best model\")\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "        # if f1_score > best_f1_score:\n",
    "        #     best_f1_score = f1_score\n",
    "        #     torch.save(model.state_dict(), \"best_retina.pth\")\n",
    "        #     print(\"Saved new best model\")\n",
    "\n",
    "    print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Current GPU memory usage: 0.36 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 6.45 GiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 22.01 GiB is allocated by PyTorch, and 92.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 36\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, lr_scheduler, train_loader, val_loader, num_epochs, device)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# # Forward pass\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# loss_dict = model(images, targets)\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# losses = sum(loss for loss in loss_dict.values())\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# optimizer.step()\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# train_loss += losses.item()\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast(device_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m---> 36\u001b[0m     loss_dict \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(loss \u001b[38;5;28;01mfor\u001b[39;00m loss \u001b[38;5;129;01min\u001b[39;00m loss_dict\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[0;32m     39\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(losses)\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\nhdan\\.conda\\envs\\ai\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nhdan\\.conda\\envs\\ai\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nhdan\\.conda\\envs\\ai\\Lib\\site-packages\\torchvision\\models\\detection\\retinanet.py:649\u001b[0m, in \u001b[0;36mRetinaNet.forward\u001b[1;34m(self, images, targets)\u001b[0m\n\u001b[0;32m    646\u001b[0m         torch\u001b[38;5;241m.\u001b[39m_assert(\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtargets should not be none when in training mode\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    647\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    648\u001b[0m         \u001b[38;5;66;03m# compute the losses\u001b[39;00m\n\u001b[1;32m--> 649\u001b[0m         losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manchors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    651\u001b[0m     \u001b[38;5;66;03m# recover level sizes\u001b[39;00m\n\u001b[0;32m    652\u001b[0m     num_anchors_per_level \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m3\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m features]\n",
      "File \u001b[1;32mc:\\Users\\nhdan\\.conda\\envs\\ai\\Lib\\site-packages\\torchvision\\models\\detection\\retinanet.py:504\u001b[0m, in \u001b[0;36mRetinaNet.compute_loss\u001b[1;34m(self, targets, head_outputs, anchors)\u001b[0m\n\u001b[0;32m    499\u001b[0m         matched_idxs\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m    500\u001b[0m             torch\u001b[38;5;241m.\u001b[39mfull((anchors_per_image\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m),), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64, device\u001b[38;5;241m=\u001b[39manchors_per_image\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    501\u001b[0m         )\n\u001b[0;32m    502\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 504\u001b[0m     match_quality_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mbox_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbox_iou\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtargets_per_image\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mboxes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manchors_per_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m     matched_idxs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproposal_matcher(match_quality_matrix))\n\u001b[0;32m    507\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead\u001b[38;5;241m.\u001b[39mcompute_loss(targets, head_outputs, anchors, matched_idxs)\n",
      "File \u001b[1;32mc:\\Users\\nhdan\\.conda\\envs\\ai\\Lib\\site-packages\\torchvision\\ops\\boxes.py:287\u001b[0m, in \u001b[0;36mbox_iou\u001b[1;34m(boxes1, boxes2)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing():\n\u001b[0;32m    286\u001b[0m     _log_api_usage_once(box_iou)\n\u001b[1;32m--> 287\u001b[0m inter, union \u001b[38;5;241m=\u001b[39m \u001b[43m_box_inter_union\u001b[49m\u001b[43m(\u001b[49m\u001b[43mboxes1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboxes2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    288\u001b[0m iou \u001b[38;5;241m=\u001b[39m inter \u001b[38;5;241m/\u001b[39m union\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m iou\n",
      "File \u001b[1;32mc:\\Users\\nhdan\\.conda\\envs\\ai\\Lib\\site-packages\\torchvision\\ops\\boxes.py:263\u001b[0m, in \u001b[0;36m_box_inter_union\u001b[1;34m(boxes1, boxes2)\u001b[0m\n\u001b[0;32m    260\u001b[0m lt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(boxes1[:, \u001b[38;5;28;01mNone\u001b[39;00m, :\u001b[38;5;241m2\u001b[39m], boxes2[:, :\u001b[38;5;241m2\u001b[39m])  \u001b[38;5;66;03m# [N,M,2]\u001b[39;00m\n\u001b[0;32m    261\u001b[0m rb \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmin(boxes1[:, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m2\u001b[39m:], boxes2[:, \u001b[38;5;241m2\u001b[39m:])  \u001b[38;5;66;03m# [N,M,2]\u001b[39;00m\n\u001b[1;32m--> 263\u001b[0m wh \u001b[38;5;241m=\u001b[39m \u001b[43m_upcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclamp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [N,M,2]\u001b[39;00m\n\u001b[0;32m    264\u001b[0m inter \u001b[38;5;241m=\u001b[39m wh[:, :, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m wh[:, :, \u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# [N,M]\u001b[39;00m\n\u001b[0;32m    266\u001b[0m union \u001b[38;5;241m=\u001b[39m area1[:, \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m+\u001b[39m area2 \u001b[38;5;241m-\u001b[39m inter\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 6.45 GiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 22.01 GiB is allocated by PyTorch, and 92.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, lr_scheduler, train_loader, val_loader, NUM_EPOCHS, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
